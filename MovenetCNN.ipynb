{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf_845oQ07_k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBUFkoRX0JGB",
        "outputId": "204b7f0e-8a7f-4f9e-9f52-679a5ddf4cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your browser has been opened to visit:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?client_id=309331432231-6narmt12c6nugdn3mum7s18r1r6m5etp.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&access_type=offline&response_type=code\n",
            "\n",
            "Authentication successful.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# import cv2\n",
        "import cv2\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "gauth= GoogleAuth()\n",
        "gauth.LocalWebserverAuth()\n",
        "drive =  GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e144kHRI1Dh9",
        "outputId": "3306f62f-d85a-4d2d-9e51-42d09edfa2fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f65069c0dcca>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Load the TensorFlow Lite model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lite-model_movenet_singlepose_thunder_3.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    447\u001b[0m       ]\n\u001b[1;32m    448\u001b[0m       self._interpreter = (\n\u001b[0;32m--> 449\u001b[0;31m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[0m\u001b[1;32m    450\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open 'lite-model_movenet_singlepose_thunder_3.tflite'."
          ]
        }
      ],
      "source": [
        "EDGES = {\n",
        "    (0, 1): 'm',\n",
        "    (0, 2): 'c',\n",
        "    (1, 3): 'm',\n",
        "    (2, 4): 'c',\n",
        "    (0, 5): 'm',\n",
        "    (0, 6): 'c',\n",
        "    (5, 7): 'm',\n",
        "    (7, 9): 'm',\n",
        "    (6, 8): 'c',\n",
        "    (8, 10): 'c',\n",
        "    (5, 6): 'y',\n",
        "    (5, 11): 'm',\n",
        "    (6, 12): 'c',\n",
        "    (11, 12): 'y',\n",
        "    (11, 13): 'm',\n",
        "    (13, 15): 'm',\n",
        "    (12, 14): 'c',\n",
        "    (14, 16): 'c'\n",
        "}\n",
        "\n",
        "# Load the TensorFlow Lite model\n",
        "interpreter = tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_thunder_3.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Define the function to draw keypoints and connections\n",
        "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
        "\n",
        "    for kp in shaped:\n",
        "        ky, kx, kp_conf = kp\n",
        "        if kp_conf > confidence_threshold:\n",
        "            cv2.circle(frame, (int(kx), int(ky)), 4, (0, 256, 0), -1)\n",
        "\n",
        "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
        "    y, x, c = frame.shape\n",
        "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1]))\n",
        "\n",
        "    for edge, color in edges.items():\n",
        "        p1, p2 = edge\n",
        "        y1, x1, c1 = shaped[p1]\n",
        "        y2, x2, c2 = shaped[p2]\n",
        "\n",
        "        if (c1 > confidence_threshold) and (c2 > confidence_threshold):\n",
        "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 256), 2)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBYzCpBP_PJ3",
        "outputId": "cd67c913-0784-4e46-de06-730de782ac41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "video_folder_path = '/content/drive/MyDrive/Files'  # Orig\n",
        "\n",
        "video_paths = []\n",
        "video_labels = []\n",
        "\n",
        "for root, dirs, files in os.walk(video_folder_path):\n",
        "    for file in files:\n",
        "        if file.endswith('.mp4'):  # Adjust the file extension if necessary\n",
        "            video_paths.append(os.path.join(root, file))\n",
        "            # Extract the label from the folder name\n",
        "            video_labels.append(os.path.basename(root))\n",
        "print(len(video_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGnLNjspBskR",
        "outputId": "e747b371-a5ab-4607-aa6c-d6fda22c9ad8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "  # Load the TensorFlow Lite model (MoveNet)\n",
        "  interpreter = tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_thunder_3.tflite')\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "pose_keypoints_list = []\n",
        "labels_list = []\n",
        "\n",
        "# Parameters\n",
        "frames_per_second = 2  # Number of frames per second to process\n",
        "minimum_video_length = 12  # Minimum length of the video in seconds\n",
        "print(len(video_labels))\n",
        "print(len(video_paths))\n",
        "# Process each video in the dataset\n",
        "count = 0\n",
        "for video_path, video_label in zip(video_paths, video_labels):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Could not open video: {video_path}\")\n",
        "        continue\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    print(video_fps)\n",
        "    if video_fps == 0:\n",
        "        print('continue')\n",
        "        continue\n",
        "    count += 1\n",
        "    print(count)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    video_length = frame_count / video_fps\n",
        "\n",
        "    # Calculate the number of frames to process based on the desired frames per second and minimum video length\n",
        "    total_frames_video = frames_per_second * minimum_video_length\n",
        "\n",
        "    # Calculate the frame skip value based on the desired frames per second\n",
        "    frame_skip = int(video_fps / frames_per_second)\n",
        "\n",
        "    # Create an empty array to store the pose keypoints for the video frames\n",
        "    video_pose_keypoints = np.zeros((total_frames_video, 17, 3))\n",
        "\n",
        "    # Process each frame in the video\n",
        "    frame_index = 0\n",
        "    for i in range(total_frames_video):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "        ret, frame = cap.read()\n",
        "        if frame is None:\n",
        "            break\n",
        "\n",
        "        # Reshape image\n",
        "        img = frame.copy()\n",
        "        img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 256, 256)\n",
        "        input_image = tf.cast(img, dtype=tf.float32)\n",
        "\n",
        "        # Setup input and output\n",
        "        input_details = interpreter.get_input_details()\n",
        "        output_details = interpreter.get_output_details()\n",
        "\n",
        "        # Make predictions\n",
        "        interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
        "        interpreter.invoke()\n",
        "        keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "        # Store the pose keypoints for the frame\n",
        "        video_pose_keypoints[i] = keypoints_with_scores[0]\n",
        "\n",
        "        # Increment the frame index by the frame skip value\n",
        "        frame_index += frame_skip\n",
        "\n",
        "    cap.release()\n",
        "    print(video_pose_keypoints.shape)\n",
        "    pose_keypoints_list.append(video_pose_keypoints)\n",
        "    labels_list.append(video_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd0MM4v9DG_p"
      },
      "outputs": [],
      "source": [
        "np.save(\"pose_keypoints_list_thunder.npy\", pose_keypoints_list)\n",
        "np.save(\"labels_list_thunder.npy\", labels_list)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}